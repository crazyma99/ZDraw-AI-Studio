# -*- coding: utf-8 -*-
"""
æ¨ç†å¼•æ“ (APIé€‚é…ç‰ˆ)
è´Ÿè´£æ¨¡å‹çš„åŠ è½½ã€æ˜¾å­˜ä¼˜åŒ–åŠå›¾ç‰‡ç”Ÿæˆã€‚
è¿”å›ç»“æ„åŒ–æ•°æ®è€Œé UI å­—ç¬¦ä¸²ã€‚
"""
import torch
from diffusers import DiffusionPipeline # type: ignore
import gc
import time
import os
from core.utils import detect_device, get_torch_dtype
from core.lora_manager import LoRAMerger
import config

class ZImageEngine:
    def __init__(self):
        self.pipe = None
        self.device = None
        self.dtype = None
        self.lora_merger = None
        self.current_lora_applied = False
        self.current_lora_configs = []

    def is_loaded(self):
        return self.pipe is not None

    def load_model(self):
        """åŠ è½½æ¨¡å‹ (è‡ªåŠ¨æ£€æµ‹è®¾å¤‡)"""
        # --- è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ£€æµ‹ ---
        if not os.path.exists(config.MODEL_PATH) or not os.listdir(config.MODEL_PATH):
            print(f"âš ï¸ [Engine] æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œæ­£åœ¨ä» ModelScope ä¸‹è½½ Tongyi-MAI/Z-Image-Turbo...")
            print(f"   ç›®æ ‡è·¯å¾„: {config.MODEL_PATH}")
            try:
                # ä¼˜å…ˆå°è¯•ä½¿ç”¨ Python API
                from modelscope import snapshot_download
                snapshot_download('Tongyi-MAI/Z-Image-Turbo', local_dir=config.MODEL_PATH)
                print("âœ… [Engine] æ¨¡å‹ä¸‹è½½å®Œæˆã€‚")
            except ImportError:
                print("âš ï¸ [Engine] æœªæ£€æµ‹åˆ° modelscope åº“ï¼Œå°è¯•ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·...")
                try:
                    subprocess.run(
                        ["modelscope", "download", "--model", "Tongyi-MAI/Z-Image-Turbo", "--local_dir", config.MODEL_PATH],
                        check=True
                    )
                except Exception as cmd_e:
                     return False, f"æ¨¡å‹ç¼ºå¤±ä¸”æ— æ³•è‡ªåŠ¨ä¸‹è½½: {str(cmd_e)}"
            except Exception as e:
                return False, f"æ¨¡å‹ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"

        self.device = detect_device()
        self.dtype = torch.bfloat16
        
        print(f"ğŸš€ [Engine] æ­£åœ¨åŠ è½½æ¨¡å‹... è®¾å¤‡: {self.device.upper()}, ç²¾åº¦: {self.dtype}")
        
        # æ¸…ç†æ—§æ˜¾å­˜
        if self.pipe:
            del self.pipe
            self.pipe = None
            gc.collect()
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            if torch.backends.mps.is_available(): torch.mps.empty_cache()

        try:
            self.pipe = DiffusionPipeline.from_pretrained(
                config.MODEL_PATH,
                torch_dtype=self.dtype,
                trust_remote_code=True,
            )
            self.pipe.to(self.device)
            
            self.lora_merger = LoRAMerger(self.pipe)
            self.current_lora_applied = False
            self.current_lora_configs = []
            
            self._apply_optimizations()
            
            print("âœ… [Engine] æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")
            return True, f"å°±ç»ª ({self.device.upper()})"
            
        except Exception as e:
            print(f"âŒ [Engine] åŠ è½½å¤±è´¥: {e}")
            return False, str(e)

    def _apply_optimizations(self):
        """åº”ç”¨ä¼˜åŒ–ç­–ç•¥"""
        # VAE å¼ºåˆ¶ FP32
        if hasattr(self.pipe, "vae"):
            self.pipe.vae.to(dtype=torch.float32) # pyright: ignore[reportOptionalMemberAccess]
            self.pipe.vae.config.force_upcast = True # pyright: ignore[reportOptionalMemberAccess]

        # ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–
        if self.device == "mps":
            # MPS æ˜¾å­˜è¶³å¤Ÿæ—¶å…³é—­ Tiling ä»¥è·å¾—æœ€ä½³ç”»è´¨
            pass 
        elif self.device == "cuda":
            self.pipe.enable_model_cpu_offload() # pyright: ignore[reportOptionalMemberAccess]
            if hasattr(self.pipe, "enable_vae_tiling"):
                self.pipe.enable_vae_tiling() # pyright: ignore[reportOptionalMemberAccess]

    def update_lora(self, enable, lora_configs):
        """
        æ›´æ–° LoRA çŠ¶æ€ (å¢é‡æ›´æ–°ï¼Œæ— éœ€é‡è½½æ¨¡å‹)
        lora_configs: list of dict {'name': str, 'scale': float}
        """
        if not self.is_loaded(): return
        
        target_configs = lora_configs if enable else []
        
        # è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿æ¯”è¾ƒ: {name: scale}
        current_map = {c['name']: c['scale'] for c in self.current_lora_configs}
        target_map = {c['name']: c['scale'] for c in target_configs}
        
        # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦å®Œå…¨ä¸€è‡´
        if current_map == target_map:
            return

        print("ğŸ”„ [Engine] æ£€æµ‹åˆ° LoRA å˜æ›´ï¼Œæ­£åœ¨åº”ç”¨å¢é‡...")
        
        # è·å–æ‰€æœ‰æ¶‰åŠçš„ LoRA åç§°
        all_names = set(current_map.keys()) | set(target_map.keys())
        
        try:
            changes_count = 0
            for name in all_names:
                old_scale = current_map.get(name, 0.0)
                new_scale = target_map.get(name, 0.0)
                diff = new_scale - old_scale
                
                if abs(diff) > 1e-4: # å¿½ç•¥å¾®å°æµ®ç‚¹è¯¯å·®
                    lora_path = os.path.join(config.LORA_DIR, name)
                    self.lora_merger.apply_lora_weights(lora_path, diff)
                    changes_count += 1
            
            if changes_count > 0:
                print(f"âœ… [Engine] LoRA æ›´æ–°å®Œæˆ ({changes_count} ä¸ªå˜åŠ¨)")
            
            # æ›´æ–°å½“å‰çŠ¶æ€
            self.current_lora_configs = target_configs
            self.current_lora_applied = bool(target_configs)
            
        except Exception as e:
            print(f"âŒ [Engine] LoRA å¢é‡æ›´æ–°å¤±è´¥: {e}")
            print("âš ï¸ [Engine] æ­£åœ¨å°è¯•å›é€€åˆ°å…¨é‡é‡è½½æ¨¡å¼...")
            
            # å›é€€æœºåˆ¶ï¼šé‡æ–°åŠ è½½æ¨¡å‹å¹¶åº”ç”¨ç›®æ ‡é…ç½®
            self.load_model()
            if target_configs:
                for config_item in target_configs:
                     lora_path = os.path.join(config.LORA_DIR, config_item['name'])
                     if os.path.exists(lora_path):
                         self.lora_merger.load_lora_weights(lora_path, config_item['scale'])
                self.current_lora_configs = target_configs
                self.current_lora_applied = True
            else:
                self.current_lora_configs = []
                self.current_lora_applied = False

    def generate(self, prompt, neg_prompt, steps, cfg, width, height, seed, seed_mode, progress_callback=None):
        """
        ç”Ÿæˆå›¾ç‰‡
        Returns:
        dict: { "image": PIL_Image, "seed": int, "duration": float }
        """
        start_time = time.time()
        
        # æ˜¾å­˜æ¸…ç†
        gc.collect()
        if self.device == "mps": torch.mps.empty_cache()
        if self.device == "cuda": torch.cuda.empty_cache()

        # ç§å­å¤„ç†
        if seed_mode == "random" or seed == -1:
            actual_seed = torch.randint(0, 2**32 - 1, (1,)).item()
        else:
            actual_seed = int(seed)
            
        gen_device = "cpu" if self.device == "mps" else self.device
        generator = torch.Generator(gen_device).manual_seed(actual_seed) # pyright: ignore[reportArgumentType]

        # æ‰“å°å½“å‰ç”Ÿæ•ˆçš„ LoRA ä¿¡æ¯
        if self.current_lora_configs:
            lora_info = ", ".join([f"{c['name']}({c['scale']})" for c in self.current_lora_configs])
            print(f"ğŸ¨ [Generate] æ­£åœ¨ä½¿ç”¨ LoRA: {lora_info}")
        else:
            print(f"ğŸ¨ [Generate] æœªå¯ç”¨ LoRA")

        print(f"ğŸ¨ [Generate] å°ºå¯¸: {width}x{height} | æ­¥æ•°: {steps} | ç§å­: {actual_seed}")

        # å›è°ƒå‡½æ•°å°è£…
        def step_callback(pipe, step_index, timestep, callback_kwargs):
            if progress_callback:
                progress_callback(step_index, steps)
            return callback_kwargs

        try:
            image = self.pipe(
                prompt=prompt,
                negative_prompt=neg_prompt,
                num_inference_steps=steps,
                guidance_scale=cfg,
                width=width,
                height=height,
                generator=generator,
                callback_on_step_end=step_callback
            ).images[0] # type: ignore
            
            duration = time.time() - start_time
            
            return {
                "success": True,
                "image": image,
                "seed": actual_seed,
                "duration": round(duration, 2)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }